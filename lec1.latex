\section{Lecture 1: Introduction to Reinforcement
Learning}\label{lecture-1-introduction-to-reinforcement-learning}

\subsection{What is reinforcement
learning?}\label{what-is-reinforcement-learning}

\subsubsection{Not supervised learning}\label{not-supervised-learning}

\begin{itemize}
\tightlist
\item
  There's no supervisor, only a received \textbf{reward signal} (that
  can be delayed).
\item
  Possibly no pre-existing dataset; agent's actions influence
  environment/data dynamically (\emph{read}: sequential, non-I.I.D.).
\end{itemize}

\subsubsection{Concepts}\label{concepts}

\begin{itemize}
\tightlist
\item
  \textbf{Reward} \(R_t\): Scalar feedback signal at time \(t\). The
  agent's goal is to maximize cumulative reward.
\item
  \textbf{History}: Sequence of \textbf{actions} \(A_t\),
  \textbf{observations} \(O_t\), and \textbf{rewards} \(R_t\) up to time
  \(t\). At each step \(t\):
\end{itemize}

\begin{verbatim}
graph LR
A[Agent] -- Action --> B[Environment]
B[Agent] -- Observation --> A[Environment]
B[Agent] -- Reward --> A[Environment]
\end{verbatim}

But, history can be noisy. It might not contain useful information or be
relevant.

\subsection{States}\label{states}

\textbf{States} are information used by the agent to determine the next
action. \[S_t = f(H_t)\]

Real quick, a state is \textbf{Markov} if and only if\\
\[ P[S_{t+1} | S_t] = P[S_{t+1} | S_1, \dots, S_t]\]

That is, \emph{the future is independent of the history, given the
present.} Technically, the entire history is a Markov state, albeit not
a very useful one.

The \textbf{environment state} \(S^e_t\) is a private representation of
data used by the environment to make decisions. It's usually not visible
to the agent, and is Markov.

The \textbf{agent state} \(S^a_t\) is a representation of data used by
the agent to pick the next action. This is used by RL algorithms.

\subsection{Environments}\label{environments}

In a \textbf{fully observable environment}, the agent observes the
complete environment. This is also known as a \textbf{Markov Decision
Process} (MDP). \[O_t = S^a_t = S^e_t\]

In a \textbf{partially observable environment}, the agent indirectly
observes the environment and creates its own \(S^a_t\). This is also
known as a \textbf{Partially Observable Markov Decision Process}
(POMDP).

\subsection{Reinforcement learning
agents}\label{reinforcement-learning-agents}

A \textbf{policy} is an agent's behavior function, which maps a
\emph{state} to an \emph{action}.

\begin{itemize}
\tightlist
\item
  Deterministic: \(a = \pi_s\)
\item
  Stochastic: \(\pi(a|s) = P[A_t = a|S_t =s]\)
\end{itemize}

A \textbf{value function} is a prediction of future reward. How good is
each state and/or action?

A \textbf{model} is an agent's representation of an environment. There
are two models:

\begin{itemize}
\tightlist
\item
  \textbf{Transition} models predict the next state given current
  reward.
\item
  \textbf{Reward} models predict the next reward given current state.
\end{itemize}

\subsubsection{Types of agents}\label{types-of-agents}

\begin{itemize}
\tightlist
\item
  \textbf{Value-based} agents have a value function and implicit policy.
\item
  \textbf{Policy-based} agents don't have a value function.
\item
  \textbf{Actor-critic} agents have both a value function and policy.
\end{itemize}

\subsubsection{Types of RL problems}\label{types-of-rl-problems}

\begin{itemize}
\tightlist
\item
  \textbf{Model-free} learning don't explicitly understand an
  environment (no model). They directly use a value function or policy.
\item
  \textbf{Model-based} learning builds up a model initially.
\end{itemize}

\subsection{Reinforcement learning
subproblems}\label{reinforcement-learning-subproblems}

\begin{itemize}
\tightlist
\item
  \textbf{Learning}: The environment/model is unknown. The agent
  interacts with an environment and improves its policy.
\item
  \textbf{Planning}: The environment/model is fully known. The agent
  performs computations (no external interaction).
\item
  \textbf{Explore/exploit}: I really like Chapter 2 of
  \href{https://bit.ly/2OGFqXF}{Algorithms to Live By} for the intuition
  behind this.
\end{itemize}

\subsection{Reference}\label{reference}

\begin{itemize}
\tightlist
\item
  \href{https://www.youtube.com/watch?v=2pWv7GOvuf0}{Video lecture}
\item
  \href{http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf}{Slides}
\end{itemize}
